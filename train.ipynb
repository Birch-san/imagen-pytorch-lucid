{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/birch/git/imagen-pytorch-lucid/train.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=62'>63</a>\u001b[0m \u001b[39m# feed images into imagen, training each unet in the cascade\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=64'>65</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=65'>66</a>\u001b[0m     loss \u001b[39m=\u001b[39m trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=66'>67</a>\u001b[0m         images,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=67'>68</a>\u001b[0m         text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=68'>69</a>\u001b[0m         text_masks \u001b[39m=\u001b[39;49m text_masks,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=69'>70</a>\u001b[0m         unet_number \u001b[39m=\u001b[39;49m i,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=70'>71</a>\u001b[0m         max_batch_size \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m        \u001b[39m# auto divide the batch of 64 up into batch size of 4 and accumulate gradients, so it all fits in memory\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=71'>72</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=73'>74</a>\u001b[0m     trainer\u001b[39m.\u001b[39mupdate(unet_number \u001b[39m=\u001b[39m i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=75'>76</a>\u001b[0m \u001b[39m# do the above for many many many many steps\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/birch/git/imagen-pytorch-lucid/train.ipynb#ch0000000?line=76'>77</a>\u001b[0m \u001b[39m# now you can sample an image based on the text embeddings from the cascading ddpm\u001b[39;00m\n",
      "File \u001b[0;32m~/git/imagen-pytorch-lucid/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/imagen-pytorch-lucid/imagen_pytorch/trainer.py:90\u001b[0m, in \u001b[0;36mcast_torch_tensor.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m args, kwargs_values \u001b[39m=\u001b[39m all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n\u001b[1;32m     88\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(kwargs_keys, kwargs_values)))\n\u001b[0;32m---> 90\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/git/imagen-pytorch-lucid/imagen_pytorch/trainer.py:396\u001b[0m, in \u001b[0;36mImagenTrainer.forward\u001b[0;34m(self, unet_number, max_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mfor\u001b[39;00m chunk_size_frac, (chunked_args, chunked_kwargs) \u001b[39min\u001b[39;00m split_args_and_kwargs(\u001b[39m*\u001b[39margs, split_size \u001b[39m=\u001b[39m max_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    395\u001b[0m     \u001b[39mwith\u001b[39;00m autocast(enabled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp):\n\u001b[0;32m--> 396\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimagen(\u001b[39m*\u001b[39;49mchunked_args, unet_number \u001b[39m=\u001b[39;49m unet_number, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mchunked_kwargs)\n\u001b[1;32m    397\u001b[0m         loss \u001b[39m=\u001b[39m loss \u001b[39m*\u001b[39m chunk_size_frac\n\u001b[1;32m    399\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/git/imagen-pytorch-lucid/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/imagen-pytorch-lucid/imagen_pytorch/imagen_pytorch.py:1956\u001b[0m, in \u001b[0;36mImagen.forward\u001b[0;34m(self, images, texts, text_embeds, text_masks, unet_number, cond_images)\u001b[0m\n\u001b[1;32m   1953\u001b[0m unet_number \u001b[39m=\u001b[39m default(unet_number, \u001b[39m1\u001b[39m)\n\u001b[1;32m   1954\u001b[0m unet_index \u001b[39m=\u001b[39m unet_number \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1956\u001b[0m unet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_unet(unet_number)\n\u001b[1;32m   1958\u001b[0m noise_scheduler      \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnoise_schedulers[unet_index]\n\u001b[1;32m   1959\u001b[0m p2_loss_weight_gamma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp2_loss_weight_gamma[unet_index]\n",
      "File \u001b[0;32m~/git/imagen-pytorch-lucid/imagen_pytorch/imagen_pytorch.py:1713\u001b[0m, in \u001b[0;36mImagen.get_unet\u001b[0;34m(self, unet_number)\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_unet\u001b[39m(\u001b[39mself\u001b[39m, unet_number):\n\u001b[0;32m-> 1713\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m unet_number \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munets)\n\u001b[1;32m   1714\u001b[0m     index \u001b[39m=\u001b[39m unet_number \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1715\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munets[index]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "\n",
    "# unet for imagen\n",
    "\n",
    "unet1 = Unet(\n",
    "    # dim = 128,\n",
    "    # dim_mults = (1, 2, 4, 8),\n",
    "    # num_resnet_blocks = 3,\n",
    "    # layer_attns = (False, True, True, True),\n",
    "    attn_dim_head = 64,\n",
    "    attn_heads = 8,\n",
    "    attn_pool_num_latents = 32,\n",
    "    attn_pool_text = False,\n",
    "    cond_on_text = True,\n",
    "    dim_mults = [1, 2, 3, 4],\n",
    "    dim = 192,\n",
    "    ff_mult = 2.0,\n",
    "    final_conv_kernel_size = 3,\n",
    "    final_resnet_block = True,\n",
    "    init_conv_to_final_conv_residual = False,\n",
    "    layer_attns = [False, True, True, True],\n",
    "    layer_cross_attns = [False, True, True, True],\n",
    "    learned_sinu_pos_emb = True,\n",
    "    lowres_cond = False,\n",
    "    memory_efficient = False,\n",
    "    num_resnet_blocks = 2,\n",
    "    resnet_groups = 8,\n",
    "    scale_resnet_skip_connection = False,\n",
    "    use_global_context_attn = False,\n",
    "    use_linear_attn = False,\n",
    ")\n",
    "\n",
    "# unet2 = Unet(\n",
    "#     dim = 128,\n",
    "#     cond_dim = 512,\n",
    "#     dim_mults = (1, 2, 4, 8),\n",
    "#     num_resnet_blocks = (2, 4, 8, 8),\n",
    "#     layer_attns = (False, False, False, True),\n",
    "#     layer_cross_attns = (False, False, False, True)\n",
    "# )\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1,),#, unet2),\n",
    "    text_encoder_name = 't5-large',\n",
    "    image_sizes = (64,),#, 256),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ")#.cuda()\n",
    "\n",
    "# wrap imagen with the trainer class\n",
    "\n",
    "trainer = ImagenTrainer(imagen)\n",
    "\n",
    "# mock images (get a lot of this) and text encodings from large T5\n",
    "\n",
    "text_embeds = torch.randn(64, 256, 1024)#.cuda()\n",
    "text_masks = torch.ones(64, 256).bool()#.cuda()\n",
    "images = torch.randn(64, 3, 256, 256)#.cuda()\n",
    "\n",
    "# feed images into imagen, training each unet in the cascade\n",
    "\n",
    "for i in (1, 2):\n",
    "    loss = trainer(\n",
    "        images,\n",
    "        text_embeds = text_embeds,\n",
    "        text_masks = text_masks,\n",
    "        unet_number = i,\n",
    "        max_batch_size = 4        # auto divide the batch of 64 up into batch size of 4 and accumulate gradients, so it all fits in memory\n",
    "    )\n",
    "\n",
    "    trainer.update(unet_number = i)\n",
    "\n",
    "# do the above for many many many many steps\n",
    "# now you can sample an image based on the text embeddings from the cascading ddpm\n",
    "\n",
    "images = trainer.sample(texts = [\n",
    "    'a puppy looking anxiously at a giant donut on the table',\n",
    "    'the milky way galaxy in the style of monet'\n",
    "], cond_scale = 3.)\n",
    "\n",
    "images.shape # (2, 3, 256, 256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bb8bfb34db8876e317cb6587a913eaadfdfd150a69853435780aeeb67ba91b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
